// Analysis helpers extracted from background.js

export function computeHeuristicScore(text) {
  const t = (text || '').trim();
  if (!t) {
    return { score: 50, confidence: 10, reasons: ['empty_text'] };
  }
  const reasons = [];
  const aiEvidence = { words: [], lines: [] };
  const humanEvidence = { words: [], lines: [] };

  const lower = t.toLowerCase();
  const length = t.length;
  const sentences = t.split(/[.!?\n]+/).filter(s => s.trim().length > 0);
  const avgLen = sentences.length ? length / sentences.length : length;

  const buzzwords = [
    'optimize', 'leverage', 'scalable', 'synergy', 'framework', 'systems', 'clarity', 'compound',
    'unlock', 'domain', 'roadmap', 'best practices', 'impactful', 'pipeline', 'actionable', 'mindset'
  ];
  const clicheOpeners = [
    "here's the thing", 'the key is', 'what i learned', "what i've learned", 'let me share',
    'in today\'s world', 'at the end of the day'
  ];
  const explicitAI = [
    'chatgpt', 'gpt-4', 'gpt4', 'gpt-3.5', 'openai', 'ollama', 'llama', 'copilot', 'bard', 'gemini',
    'as an ai language model', 'generated by ai', 'ai wrote this', 'prompted with', ' ai '
  ];

  const emojiPattern = /[\u{1F300}-\u{1FAFF}]/u;
  const quotationFancy = /[“”]/;
  const fullFancyWrap = /^“[\s\S]*”$/;
  const dashPattern = /[—–]/g; // em/en dash
  const mdBullets = /(^|\n)\s*(?:\d+\.|\-|\*)\s/m;
  const properRun = /[A-Z][a-z]+(\s[A-Z][a-z]+){3,}/;

  // Evidence counters
  let aiE = 0; // AI-leaning evidence
  let huE = 0; // Human-leaning evidence

  const buzzCount = buzzwords.reduce((acc, w) => acc + (lower.includes(w) ? 1 : 0), 0);
  if (buzzCount) {
    aiE += Math.min(3, buzzCount);
    reasons.push('buzzwords');
    // capture matched buzzwords
    buzzwords.forEach(w => { if (lower.includes(w)) aiEvidence.words.push({ word: w, reason: 'buzzword/consulting speak' }); });
  }
  if (clicheOpeners.some(p => lower.startsWith(p))) {
    aiE += 2; reasons.push('cliche_opener');
    const openerStr = clicheOpeners.find(p => lower.startsWith(p));
    if (openerStr) aiEvidence.lines.push({ text: sentences[0]?.trim() || openerStr, reason: 'cliche opener' });
  }
  if (emojiPattern.test(t)) { aiE += 1; reasons.push('emoji_pattern'); aiEvidence.words.push({ word: 'emoji', reason: 'excess emoji pattern' }); }
  if (quotationFancy.test(t)) { aiE += 1; reasons.push('fancy_quotes'); aiEvidence.words.push({ word: '“ ”', reason: 'fancy quotes' }); }
  if (fullFancyWrap.test(t)) { aiE += 2; reasons.push('full_fancy_quotes_wrap'); aiEvidence.lines.push({ text: t, reason: 'wrapped in smart quotes' }); }
  const dashes = t.match(dashPattern);
  if (dashes && dashes.length) { aiE += Math.min(2, dashes.length); reasons.push('em_dash_usage'); aiEvidence.words.push({ word: '—', reason: 'em/en dash usage' }); }
  if (mdBullets.test(t)) {
    aiE += 2; reasons.push('listicle_format');
    const listLines = t.split(/\n+/).filter(l => /^\s*(?:\d+\.|-|\*)\s/.test(l));
    listLines.slice(0, 5).forEach(l => aiEvidence.lines.push({ text: l.trim(), reason: 'listicle/templated line' }));
  }
  if (avgLen > 140) { aiE += 1; reasons.push('overlong_sentences'); }
  if (properRun.test(t)) { aiE += 1; reasons.push('proper_noun_run'); }

  // Parallelism / patterned cadence: two short sentences of similar length and structure
  if (sentences.length === 2) {
    const s1 = sentences[0].trim();
    const s2 = sentences[1].trim();
    if (s1 && s2) {
      const lenDiff = Math.abs(s1.length - s2.length) / Math.max(1, Math.max(s1.length, s2.length));
      const tokenize = s => s.toLowerCase().replace(/[^a-z0-9\s]/g, '').split(/\s+/).filter(Boolean);
      const t1 = tokenize(s1);
      const t2 = tokenize(s2);
      const starts = [['here', 'is'], ['here', 'are'], ['sometimes'], ['then'], ['and', 'then']];
      const patternish = (w) => w.startsWith('then') || w.startsWith('sometimes') || w === 'and';
      const cadence = (t1[0] && patternish(t1[0])) || (t2[0] && patternish(t2[0]));
      if (lenDiff <= 0.25 || cadence) {
        aiE += 2; reasons.push('parallel_structure');
        aiEvidence.lines.push({ text: s1, reason: 'parallel structure' });
        aiEvidence.lines.push({ text: s2, reason: 'parallel structure' });
      }
      // numeric cadence like "... 47 ... then ... 53 ..."
      const n1 = s1.match(/\d+/g) || [];
      const n2 = s2.match(/\d+/g) || [];
      if (n1.length && n2.length && /\bthen\b/i.test(s2)) {
        aiE += 1; reasons.push('patterned_numeric_cadence');
      }
    }
  }
  if (explicitAI.some(p => lower.includes(p))) {
    aiE += 4; reasons.push('explicit_ai_mention');
    explicitAI.forEach(w => { if (lower.includes(w.trim())) aiEvidence.words.push({ word: w.trim(), reason: 'explicit AI mention' }); });
  }

  // Human evidence
  const slangMatch = t.match(/\b(gonna|wanna|ain't|idk|lol|lmao|tbh|bruh|dude|ya|tho|ngl)\b/gi);
  const typoMatch = t.match(/\b(teh|recieve|becuase|definately|seperate|occured)\b/gi);
  const specificsMatch = t.match(/\b(\d{1,2}:\d{2}|\d{4}|mon|tue|wed|thu|fri|sat|sun|mumbai|delhi|blr|nyc|london|bandra|andheri|brooklyn|queens|pune)\b/gi);
  const firstPersonMatch = t.match(/\b(i|my|we|our|yesterday|today|last night|this morning)\b/gi);
  const contractionsMatch = t.match(/\b(I'm|I've|don't|can't|won't|didn't|it's|that's|there's|you're|we're|they're)\b/gi);
  const informalPunct = t.match(/(\.\.\.|\?!|!\?|!!)/g);
  if (slangMatch) { huE += 2; reasons.push('slang'); slangMatch.slice(0,5).forEach(w => humanEvidence.words.push({ word: w, reason: 'slang/human voice' })); }
  if (typoMatch) { huE += 2; reasons.push('typos'); typoMatch.slice(0,5).forEach(w => humanEvidence.words.push({ word: w, reason: 'typo/imperfection' })); }
  if (specificsMatch) { huE += 2; reasons.push('specific_context'); specificsMatch.slice(0,5).forEach(w => humanEvidence.words.push({ word: w, reason: 'specific time/place/detail' })); }
  if (firstPersonMatch) { huE += 1; reasons.push('first_person_context'); firstPersonMatch.slice(0,5).forEach(w => humanEvidence.words.push({ word: w, reason: 'first-person context' })); }
  if (contractionsMatch) { huE += 1; reasons.push('contractions'); contractionsMatch.slice(0,4).forEach(w => humanEvidence.words.push({ word: w, reason: 'contraction' })); }
  if (informalPunct) { huE += 1; reasons.push('informal_punctuation'); humanEvidence.words.push({ word: informalPunct[0], reason: 'informal punctuation' }); }

  // Normalize to dynamic 0-100 around 50 baseline
  const total = aiE + huE;
  let score;
  if (total === 0) {
    score = 50; // no evidence
  } else {
    const diff = aiE - huE;
    // map from [-total, total] to [0,100] centered at 50
    score = 50 + (diff / total) * 40; // 40 gives room, leaving extremes to model blend
  }
  score = Math.round(Math.max(0, Math.min(100, score)));

  // Confidence based on amount and agreement of evidence and text length
  let conf = 20 + Math.min(60, total * 10);
  if (length < 40) conf -= 10;
  conf = Math.max(10, Math.min(90, Math.round(conf)));

  return { score, confidence: conf, reasons, aiEvidence, humanEvidence };
}

export function createAnalysisPrompt(tweetText) {
  return `You are an expert detector of AI-generated social media text. Analyze the tweet below and return ONLY a JSON object that quantifies AI-likeness, highlights exact phrases and full lines that are suspicious, and explains your reasoning.

Tweet:
"""
${tweetText}
"""

Evaluate using concrete evidence across these axes:
- Tone and voice consistency (robotic/overly formal, generic inspiration, salesy cadence, lack of personal voice)
- Linguistic patterns (repeated templates, listicle framing, parallelism, even punctuation, quotation style “ ” vs " ")
- Content traits (platitudes, universal advice, missing concrete details, lack of slang/cultural context, perfect grammar without natural imperfections)
- Surface signals (excess emoji patterns, cliché openers like "Here’s the thing", "What I learned", "The key is")

Scoring rubric (calibrated):
- 0–20: Strongly human signals dominate (idiosyncrasies, typos, slang, concrete specifics, temporal/spatial references, unique voice)
- 21–40: Mostly human with some generic patterns or mild templating
- 41–60: Uncertain/mixed; use only if evidence is balanced in both directions
- 61–80: Likely AI; multiple generic/templated indicators, smooth but impersonal tone, corporate buzzwords
- 81–100: Very likely AI; template phrases, listicle/lecture tone, over-structured lines, excessive emoji patterns, safe corporate speak

Calibration rules (proportional, no hard floors/ceilings):
- Increase aiScore proportionally with explicit AI-source indicators (mentions of ChatGPT/GPT/OpenAI/Gemini, templated phrasing, listicle structure, polished but impersonal tone, repetitive syntax). The more and stronger the indicators, the higher the score.
- Decrease aiScore proportionally with human signals (slang, typos, localized specifics, first-person diary-like details, spontaneous asides). The more and stronger the signals, the lower the score.
- For short, colloquial personal updates (≤ 25 words) with human signals, prefer lower scores unless strong counter-evidence exists. Avoid arbitrary midpoints.

Rules for outputs:
- aiScore must reflect the rubric and the density/strength of indicators; do NOT default to 50.
- confidence reflects evidence strength and text length (short text => lower confidence unless signals are very strong).
- suspiciousWords: up to 8 exact substrings from the tweet with short reasons.
- suspiciousLines: exact sentences/lines from the tweet that carry AI signals with concise reasons. Use precise quotes.

Few-shot guidance (examples):
// Human-leaning (score ~20-35)
Text: "missed the 8:12 at dadar again. mumbai rains and the local was PACKED. whoever designed that footbridge owes me new shoes."
Signals: slang, localized context, specific time/place, imperfections.
Outcome: aiScore 28, confidence 72

// Mixed/uncertain (score ~45-55)
Text: "Here are 3 habits that helped me focus this year: 1) plan the night before 2) deep work in the morning 3) take a walk at lunch."
Signals: listicle/template, but personal/time-bound reference. Balanced evidence.
Outcome: aiScore 52, confidence 55

// AI-leaning (score ~70-85)
Text: "Here’s the thing: Consistency compounds. If you optimize for clarity and systems, you’ll unlock exponential potential across every domain."
Signals: cliché opener, corporate buzzwords, abstract claims, templated cadence, perfect polish.
Outcome: aiScore 78, confidence 78

Return EXACTLY this JSON schema and nothing else:
{
  "aiScore": 0,
  "confidence": 0,
  "suspiciousWords": [
    { "word": "exact substring", "reason": "brief why" }
  ],
  "analysis": {
    "tone": "brief tone indicators",
    "patterns": "linguistic pattern observations",
    "content": "content trait observations"
  },
  "suspiciousLines": [
    { "text": "exact line/sentence", "reason": "brief why" }
  ],
  "reasoning": "one or two sentences summarizing the key evidence and how it maps to the rubric"
}
}`;
}

export function extractSuspiciousFromHeuristic(text) {
  const t = String(text || '');
  const lower = t.toLowerCase();
  
  const buzzwords = [
    'optimize', 'leverage', 'scalable', 'synergy', 'framework', 'systems', 'clarity', 'compound',
    'unlock', 'domain', 'roadmap', 'best practices', 'impactful', 'pipeline', 'actionable', 'mindset'
  ];
  const clicheOpeners = [
    "here's the thing", 'the key is', 'what i learned', "what i've learned", 'let me share',
    'in today\'s world', 'at the end of the day'
  ];
  const explicitAIWords = [
    'ai', 'a.i.', 'artificial intelligence', 'chatgpt', 'gpt', 'gpt-4', 'gpt4', 'openai', 'gemini', 'bard', 'copilot', 'llama'
  ];
  
  const suspiciousWords = [];
  const suspiciousLines = [];
  
  buzzwords.forEach(w => {
    if (lower.includes(w)) {
      suspiciousWords.push({ word: w, reason: 'buzzword/templated phrasing' });
    }
  });

  // Explicit AI mentions (generic or model names)
  explicitAIWords.forEach(w => {
    if (lower.includes(w)) {
      // Use the original-case match if possible; else keep lower token
      const re = new RegExp(w.replace(/[.*+?^${}()|[\]\\]/g, '\\$&'), 'i');
      const m = t.match(re);
      suspiciousWords.push({ word: m ? m[0] : w, reason: 'explicit AI mention' });
    }
  });
  
  const opener = clicheOpeners.find(p => lower.startsWith(p));
  if (opener) {
    suspiciousWords.push({ word: opener, reason: 'cliché opener template' });
  }
  
  const listLines = t.match(/(^|\n)\s*(?:\d+\.|\-|\*)\s.*$/gm);
  if (listLines) {
    listLines.slice(0, 4).forEach(line => {
      suspiciousLines.push({ text: line.trim(), reason: 'listicle/template structure' });
    });
  }
  
  // If we have no line-level matches but content likely references AI generically, add a weak line highlight
  if (suspiciousLines.length === 0 && explicitAIWords.some(w => lower.includes(w))) {
    const firstLine = t.split(/\n+/)[0].trim();
    if (firstLine) {
      suspiciousLines.push({ text: firstLine, reason: 'AI-related statement (weak structural signal)' });
    }
  }
  
  // Only use long-word fallback if we already detected AI-leaning signals
  const hasAISignals = (buzzwords.some(w => lower.includes(w)) ||
                        explicitAIWords.some(w => lower.includes(w)) ||
                        (listLines && listLines.length > 0) ||
                        opener);
  if (suspiciousWords.length === 0 && hasAISignals) {
    const tokens = Array.from(new Set(t.match(/[A-Za-z]{10,}/g) || [])).slice(0, 3);
    tokens.forEach(tok => suspiciousWords.push({ word: tok, reason: 'long abstract term (weak signal)' }));
  }
  
  return { suspiciousWords, suspiciousLines };
}
